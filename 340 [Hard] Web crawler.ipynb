{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description\n",
    "\n",
    "    Most of us are familiar with web spiders and crawlers like GoogleBot - they visit a web page, index content there, and then visit outgoing links from that page. Crawlers are an interesting technology with continuing development.\n",
    "\n",
    "    Web crawlers marry queuing and HTML parsing and form the basis of search engines etc. Writing a simple crawler is a good exercise in putting a few things together. Writing a well behaved crawler is another step up.\n",
    "    For this challenge you may use any single shot web client you wish, e.g. Python's httplib or any of a number of libcurl bindings; you may NOT use a crawling library like Mechanize or whatnot. You may use an HTML parsing library like BeautifulSoup; you may NOT use a headless browser like PhantomJS. The purpose of this challenge is to tie together fetching a page, reassembling links, discovering links and assembling them, adding them to a queue, managing the depth of the queue, and visiting them in some reasonable order - while avoiding duplicate visits.\n",
    "\n",
    "    Your crawler MUST support the following features:\n",
    "\n",
    "    HTTP/1.1 client behaviors\n",
    "    GET requests are the only method you must support\n",
    "    Parse all links presented in HTML - anchors, images, scripts, etc\n",
    "    Take at least two options - a starting (seed) URL and a maximum depth to recurse to (e.g. \"1\" would be fetch the HTML page and all resources like images and script associated with it but don't visit any outgoing anchor links; a depth of \"2\" would visit the anchor links found on that first page only, etc ...)\n",
    "\n",
    "    Do not visit the same link more than once per session\n",
    "    Optional features include HTTPS support, support for robots.txt, support for domains to which you restrict the crawler, and storing results (for example how wget does so).\n",
    "    \n",
    "    Be careful with what you crawl! Don't get yourself banned from the Internet. I highly suggest you crawl a local server you control as you may trigger rate limits and other mechanisms to identify unwanted visitors.\n",
    "\n",
    "[Reddit Challange](https://www.reddit.com/r/dailyprogrammer/comments/7dlaeq/20171117_challenge_340_hard_write_a_web_crawler/)\n",
    "\n",
    "[Extra source](http://www.east5th.co/blog/2017/10/09/learning-to-crawl-building-a-bare-bones-web-crawler-with-elixir/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import collections as co\n",
    "\n",
    "pattern = re.compile(r'''[a-z]*://|          # protocol                                             \n",
    "                         (?<=://)[a-z0-9.]*| # main_url\n",
    "                         (?<=[a-z])/.*|      # detail\n",
    "                         ^/.*                # local ref\n",
    "                         ''',re.X)\n",
    "\n",
    "def process_link(link,old=''):\n",
    "\n",
    "    found = re.findall(pattern,link)\n",
    "    \n",
    "    if len(found)>1:\n",
    "        parts = found.pop(1).split('.')\n",
    "        two_parts = ['.'.join(parts[:-2]) + '.'*bool(parts[:-2]),\n",
    "                     '.'.join(parts[-2:])]\n",
    "        d_if = {True:found.pop, False:lambda:''}   \n",
    "        found += [*two_parts, d_if[bool(len(found)>1)]()]\n",
    "    elif found:\n",
    "        found = old + found\n",
    "    else:\n",
    "        found = old\n",
    "    return found\n",
    "\n",
    "def visit(link):\n",
    "    \n",
    "    r = requests.get(link)\n",
    "    if r.status_code != 200:\n",
    "        return\n",
    "    else:\n",
    "        old = process_link(link)[:3]\n",
    "        out_links = []\n",
    "        soup = bs(r.text,'html.parser')\n",
    "        for line in soup.find_all('a'):\n",
    "            a_link = line.get('href')\n",
    "            if a_link:\n",
    "                out_links += [process_link(a_link,old)]\n",
    "        return out_links\n",
    "            \n",
    "def crawl(a_link,visits):\n",
    "    \n",
    "    l_list = process_link(a_link)\n",
    "    queue = co.deque([l_list])\n",
    "    main_visited = co.defaultdict(int) \n",
    "    main_visited[l_list[2]] += 1\n",
    "    all_visited = set()\n",
    "\n",
    "    while queue and visits:\n",
    "        \n",
    "        l_list = queue.popleft()\n",
    "        link = ''.join(l_list)\n",
    "        if not link in all_visited and main_visited[l_list[2]] < 4:\n",
    "            main_visited[l_list[2]] += 1\n",
    "            all_visited.add(link)\n",
    "            visits -= 1\n",
    "            now = visit(''.join(link))\n",
    "            queue.extend(now)\n",
    "    \n",
    "    return main_visited\n",
    "            \n",
    "# crawl('https://www.google.com',40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'ad.nl': 1,\n",
       "             'android.com': 2,\n",
       "             'blogger.com': 1,\n",
       "             'demorgen.be': 3,\n",
       "             'google.be': 4,\n",
       "             'google.com': 4,\n",
       "             'hbvl.be': 1,\n",
       "             'hln.be': 4,\n",
       "             'knack.be': 1,\n",
       "             'metrotime.be': 1,\n",
       "             'nieuwsblad.be': 4,\n",
       "             'nos.nl': 1,\n",
       "             'nrc.nl': 1,\n",
       "             'sporza.be': 1,\n",
       "             'standaard.be': 4,\n",
       "             'tijd.be': 2,\n",
       "             'vrt.be': 1,\n",
       "             'welingelichtekringen.nl': 1,\n",
       "             'youtube.com': 4})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crawl('https://www.google.com',40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "visited = set()\n",
    "\n",
    "async def crawl(node, max_depth, session):\n",
    "    if node['depth'] > max_depth:\n",
    "        return\n",
    "    node['next'] = {\n",
    "        'depth': node['depth'] + 1,\n",
    "        'urls': [],\n",
    "    }\n",
    "    for link in node['urls']:\n",
    "        links = await get_links(link, session)\n",
    "        node['next']['urls'].extend(links)\n",
    "    await crawl(node['next'], max_depth, session)\n",
    "\n",
    "async def get_links(url, session):\n",
    "    if url in visited or url.startswith('/'):\n",
    "        return []\n",
    "    visited.add(url)\n",
    "    links = []\n",
    "    async with session.get(url) as resp:\n",
    "        text = await resp.text()\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if '//' in a['href']:\n",
    "                links.append(a['href']) \n",
    "    return links\n",
    "\n",
    "async def main():\n",
    "    # max_depth = 2\n",
    "    # root = r'http://quotes.toscrape.com/'\n",
    "    root = input('Website to crawl: ')\n",
    "    max_depth = int(input('Depth to crawl (int): '))\n",
    "    links = {\n",
    "        'urls': [root],\n",
    "        'depth': 1,\n",
    "    }\n",
    "    with aiohttp.ClientSession() as session:\n",
    "        await crawl(links, max_depth, aiohttp.ClientSession())\n",
    "    print(f'Number of pages crawled: {len(visited)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website to crawl: http://quotes.toscrape.com/\n",
      "Depth to crawl (int): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001A536628748>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001A535A506D8>, 446205.906)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001A536628C50>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages crawled: 1\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to await expression (<ipython-input-45-9cdd2b45b231>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-9cdd2b45b231>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    await tp_level = self.top_level(url)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to await expression\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "# from bloom_filter import BloomFilter\n",
    "\n",
    "import collections as co\n",
    "\n",
    "\n",
    "class Crawler():\n",
    "\n",
    "    def __init__(self,source,max_visits=10,max_rude=3):\n",
    "    \n",
    "        # self.bloom = BloomFilter(max_elements=max_visits, error_rate=0.1)\n",
    "        self.done = set()\n",
    "        \n",
    "        self.n = max_visits\n",
    "        self.rude = max_rude\n",
    "\n",
    "        self.main_visited = co.defaultdict(int)\n",
    "        self.queue = co.deque([source])\n",
    "\n",
    "    # not blocking\n",
    "    async def top_level(self,url):   \n",
    "        \n",
    "        found = re.search(r'(?<=://)[a-z0-9.]*',url)\n",
    "        if found:\n",
    "            parts = found[0].split('.')\n",
    "            tp_level = '.'.join(parts[-2:])\n",
    "            await tp_level\n",
    "        await False\n",
    "\n",
    "    async def crawl(self):\n",
    "\n",
    "        while self.queue and self.n:\n",
    "            url = self.queue.popleft()\n",
    "            if url not in self.done:\n",
    "                await tp_level = self.top_level(url)\n",
    "                if self.main_visited[tp_level] < self.rude:\n",
    "                    await self.get_links(url, tp_level)\n",
    "                self.main_visited[tp_level] += 1\n",
    "                \n",
    "    async def get_links(self, url, tp_level):\n",
    "\n",
    "        try:\n",
    "            async with aiohttp.ClientSession().get(url) as resp:\n",
    "                text = await resp.text()\n",
    "                self.n -= 1\n",
    "                self.done.add(url)\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                for new_url in soup.find_all('a', href=True):\n",
    "                    c = await self.top_level(new_url['href'])\n",
    "                    if c:\n",
    "                        self.queue.append(c)\n",
    "        except:\n",
    "            await asyncio.sleep(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE7A4E3550>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE7A4E3588>, 491837.687)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE799A0DD8>, 491837.828)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE7A4E36A0>\n",
      "C:\\Users\\k\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: coroutine 'Crawler.top_level' was never awaited\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE7A276668>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE79A204E0>, 491838.031)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE79F43EF0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE7A226748>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE7A237748>, 491838.187)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE799163C8>, 491838.578)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE79C63390>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE79916F60>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE799B52B0>, 491839.14)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE79916EB8>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE798F7320>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE798F72B0>, 491841.593)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE798F7160>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE7A94A748>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE7A94A940>, 491842.671)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE79F1B630>, 491843.203)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE7A94A828>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE79F129B0>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE79F12DD8>, 491844.062)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE7A222D30>, 491844.312)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE79F12F98>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE79F12A58>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE7A94A240>, 491844.609)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE7A5C1748>, 491844.796)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE79AA9908>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE7A5C1CF8>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE79AA9908>, 491845.078)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE7A5C1828>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001BE79EE18D0>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000001BE79EE14A8>, 491845.718)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x000001BE79EE1208>\n"
     ]
    }
   ],
   "source": [
    "crow = Crawler('https://www.google.com/')\n",
    "    \n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(crow.crawl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int,\n",
       "             {<coroutine object Crawler.top_level at 0x000001BE7A650620>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE79E13CA8>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE79344C50>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE77C0DE60>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE7A650570>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE7A650A40>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE792DEDB0>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE792DED00>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE79FE0780>: 1,\n",
       "              <coroutine object Crawler.top_level at 0x000001BE7AAF70A0>: 1}),\n",
       " 0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crow.main_visited,crow.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'google.be': 2, 'google.com': 3})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(crawl('https://www.google.com',4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www', 'google', 'com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "a = re.search(r'(?<=://)[a-z0-9.]*','https://www.google.com')\n",
    "a[0].split('.')[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "16\n",
      "256\n",
      "65536\n",
      "4294967296\n",
      "18446744073709551616\n",
      "340282366920938463463374607431768211456\n",
      "115792089237316195423570985008687907853269984665640564039457584007913129639936\n",
      "13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096\n"
     ]
    }
   ],
   "source": [
    "def small():\n",
    "    yield 2\n",
    "    a = 2\n",
    "    while True:\n",
    "        a = a**2\n",
    "        yield a\n",
    "        \n",
    "g = small()\n",
    "for i in range(10):\n",
    "    print(next(g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import collections as co\n",
    "\n",
    "pattern = re.compile(r'''[a-z]*://|          # protocol                                             \n",
    "                         (?<=://)[a-z0-9.]*| # main_url\n",
    "                         (?<=[a-z])/.*|      # detail\n",
    "                         ^/.*                # local ref\n",
    "                         ''',re.X)\n",
    "\n",
    "def process_link(link,old=''):\n",
    "   \n",
    "    found = re.findall(pattern,link)\n",
    "    \n",
    "    if len(found)>1:\n",
    "        parts = found.pop(1).split('.')\n",
    "        two_parts = ['.'.join(parts[:-2]) + '.'*bool(parts[:-2]),\n",
    "                     '.'.join(parts[-2:])]\n",
    "        d_if = {True:found.pop, False:lambda:''}   \n",
    "        found += [*two_parts, d_if[bool(len(found)>1)]()]\n",
    "    elif found:\n",
    "        found = old + found\n",
    "    else:\n",
    "        found = old\n",
    "    return found\n",
    "\n",
    "async def visit(link):\n",
    "    \n",
    "    r = requests.get(link)\n",
    "    if r.status_code != 200:\n",
    "        return\n",
    "    else:\n",
    "        old = process_link(link)[:3]\n",
    "        out_links = []\n",
    "        soup = bs(r.text,'html.parser')\n",
    "        for line in soup.find_all('a'):\n",
    "            a_link = line.get('href')\n",
    "            if a_link:\n",
    "                out_links += [process_link(a_link,old)]\n",
    "        return out_links\n",
    "            \n",
    "async def crawl(a_link,visits):\n",
    "    \n",
    "    l_list = process_link(a_link)\n",
    "    queue = co.deque([l_list])\n",
    "    main_visited = co.defaultdict(int) \n",
    "    main_visited[l_list[2]] += 1\n",
    "    all_visited = set()\n",
    "\n",
    "    while queue and visits:\n",
    "        \n",
    "        l_list = queue.popleft()\n",
    "        link = ''.join(l_list)\n",
    "        if not link in all_visited and main_visited[l_list[2]] < 4:\n",
    "            main_visited[l_list[2]] += 1\n",
    "            all_visited.add(link)\n",
    "            visits -= 1\n",
    "            now = await visit(''.join(link))\n",
    "            queue.extend(now)\n",
    "    \n",
    "    return main_visited\n",
    "            \n",
    "crawl('https://www.google.com',4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
