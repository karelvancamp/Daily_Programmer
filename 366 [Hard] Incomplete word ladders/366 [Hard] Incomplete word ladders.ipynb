{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Given two different strings of equal length, the spacing between them is the number of other strings you would need to connect them on a word ladder. Alternately, this is 1 less than the number of letters that differ between the two strings. Examples:\n",
    "\n",
    "    spacing(\"shift\", \"shirt\") => 0\n",
    "    spacing(\"shift\", \"whist\") => 1\n",
    "    spacing(\"shift\", \"wrist\") => 2\n",
    "    spacing(\"shift\", \"taffy\") => 3\n",
    "    spacing(\"shift\", \"hints\") => 4\n",
    "\n",
    "The total spacing of a word list is the sum of the spacing between each consecutive pair of words on the word list, i.e. the number of (not necessarily distinct) strings you'd need to insert to make it into a word ladder. For example, the list:\n",
    "\n",
    "    daily\n",
    "    doily\n",
    "    golly\n",
    "    guilt\n",
    "\n",
    "has a total spacing of 0 + 1 + 2 = 3\n",
    "\n",
    "## Challenge\n",
    "\n",
    "Given an input list of unique words and a maximum total spacing, output a list of distinct words taken from the input list. The output list's total spacing must not exceed the given maximum. The output list should be as long as possible.\n",
    "\n",
    "You are allowed to use existing libraries and research in forming your solution. (I'm guessing there's some graph theory algorithm that solves this instantly, but I don't know it.)\n",
    "\n",
    "#### Example input\n",
    "\n",
    "    abuzz\n",
    "    carts\n",
    "    curbs\n",
    "    degas\n",
    "    fruit\n",
    "    ghost\n",
    "    jupes\n",
    "    sooth\n",
    "    weirs\n",
    "    zebra\n",
    "\n",
    "Maximum total spacing: 10\n",
    "\n",
    "#### Example output\n",
    "\n",
    "The longest possible output given this input has length of 6:\n",
    "\n",
    "    zebra\n",
    "    weirs\n",
    "    degas\n",
    "    jupes\n",
    "    curbs\n",
    "    carts\n",
    "\n",
    "#### Challenge input\n",
    "\n",
    "This list of 1000 4-letter words randomly chosen from enable1.\n",
    "\n",
    "Maximum total spacing of 100.\n",
    "\n",
    "My best solution has a length of 602. How much higher can you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Pseudo\n",
    "\n",
    "- Calculate spacing distances.\n",
    "- Remove isolated nodes (those which have no 0 cost edges).\n",
    "- Add hyper-node in order to translate Hamiltonian path problem to TSP.\n",
    "- Run TSP-solver on leftover nodes.\n",
    "- Remove hypernode, this transforms cycle into path.\n",
    "- Shorthen path until cost requirement is fulfilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:\n",
      "  1000\n"
     ]
    }
   ],
   "source": [
    "with open('366-hard-words.txt') as f:\n",
    "    file = f.read().splitlines()\n",
    "    \n",
    "print(f'Words:\\n  {len(file)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate spacing distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacing(a,b):\n",
    "    return max(sum(1 for x,y in zip(a,b) if x != y)-1,0)\n",
    "\n",
    "def distance_words(words):\n",
    "    return [[spacing(x,y) for x in words] for y in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges grouped by weight:\n",
      " {0: 3804, 1: 35618, 2: 243880, 3: 716698}\n"
     ]
    }
   ],
   "source": [
    "L = distance_words(file)\n",
    "\n",
    "D = {x:0 for x in range(4)}\n",
    "for x in L:\n",
    "    for y in x:\n",
    "        D[y] += 1\n",
    "        \n",
    "print(f'Edges grouped by weight:\\n {D}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove isolated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# redundant, was first investigating using cliques to build a 0 cost skeleton bones\n",
    "# now just use it to identify the isolated nodes\n",
    "# Could just look at distance matrix to identify them more cheaply\n",
    "\n",
    "def clique(L):\n",
    "    edges = [(ix,iy,y) for ix,x in enumerate(L) \n",
    "                 for iy,y in enumerate(x) \n",
    "                 if not y]   \n",
    "    G=nx.Graph()\n",
    "    for x,y,w in edges:\n",
    "        G.add_edge(x,y)        \n",
    "    cliques = list(nx.algorithms.clique.find_cliques(G))\n",
    "    return cliques\n",
    "\n",
    "def member(cliques):\n",
    "    members = collections.defaultdict(list)\n",
    "    for ix,x in enumerate(cliques):\n",
    "        for y in x:\n",
    "            members[y].append([z for z in x if z != y])\n",
    "    return members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliques:\n",
      "  855\n",
      "Clique sizes:\n",
      "  {1: 149, 2: 518, 3: 131, 4: 37, 5: 10, 6: 8, 7: 1, 8: 1}\n",
      "Total clique nodes:\n",
      "  1839\n",
      "Unique nodes:\n",
      "  1000\n",
      "Expensive nodes:\n",
      "  149\n"
     ]
    }
   ],
   "source": [
    "cliques = clique(L)\n",
    "members = member(cliques)\n",
    "\n",
    "measure = {x:0 for x in range(1,9)}\n",
    "for x in cliques:\n",
    "    measure[len(x)] += 1  \n",
    "\n",
    "print(f'Cliques:\\n  {len(cliques)}')\n",
    "print(f'Clique sizes:\\n  {measure}')\n",
    "print(f'Total clique nodes:\\n  {sum((y*x) for x,y in measure.items())}')\n",
    "print(f'Unique nodes:\\n  {len(members)}')\n",
    "print(f'Expensive nodes:\\n  {measure[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expensive = {x[0] for x in cliques if len(x) == 1}\n",
    "\n",
    "Lsmall = [[y for iy,y in enumerate(x) if iy not in expensive] \n",
    "          for ix,x in enumerate(L) if ix not in expensive]\n",
    "filesmall = [x for ix,x in enumerate(file) if not ix in expensive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add hyper-node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.array(Lsmall)\n",
    "hamilithon_path = np.pad(arr, [(0,1),(0,1)], \"constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run TSP-solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses win lkh.exe available in same root\n",
    "# http://www.akira.ruc.dk/~keld/research/LKH/lkh.exe\n",
    "\n",
    "template = \"\"\"NAME: {name}\n",
    "TYPE: TSP\n",
    "COMMENT: {name}\n",
    "DIMENSION: {n_cities}\n",
    "EDGE_WEIGHT_TYPE: EXPLICIT\n",
    "EDGE_WEIGHT_FORMAT: LOWER_DIAG_ROW\n",
    "EDGE_WEIGHT_SECTION\n",
    "{matrix_s}EOF\"\"\"\n",
    "\n",
    "def dumps_matrix(arr, name=\"Problem\"):\n",
    "    n_cities = arr.shape[0]\n",
    "    width = len(str(arr.max())) + 1\n",
    "    matrix_s = \"\"\n",
    "    for i, row in enumerate(arr.tolist()):\n",
    "        matrix_s += \" \".join([\"{0:>{1}}\".format((int(elem)), width)\n",
    "                              for elem in row[:i+1]])\n",
    "        matrix_s += \"\\n\"\n",
    "    return template.format(**{'name': name,\n",
    "                              'n_cities': n_cities,\n",
    "                              'matrix_s': matrix_s})\n",
    "\n",
    "def _create_lkh_par(tsp_path, runs=4):\n",
    "    par_path = tsp_path + \".par\"\n",
    "    out_path = tsp_path + \".out\"\n",
    "    par = 'PROBLEM_FILE = {}\\nRUNS = {}\\nTOUR_FILE = {}'.format(tsp_path, runs, out_path)\n",
    "    with open(par_path, 'w') as dest:\n",
    "        dest.write(par)\n",
    "    return par_path, out_path\n",
    "\n",
    "def run(tsp_file='problem.tsp',runs = 1):\n",
    "    with open(tsp_file, 'w') as problem:\n",
    "        problem.write(dumps_matrix(hamilithon_path, name=tsp_file))\n",
    "    par_path, out_path = _create_lkh_par(tsp_file, runs)\n",
    "    \n",
    "    subprocess.call(['lkh', par_path])\n",
    "\n",
    "    with open(out_path) as solution:\n",
    "        lkhout = solution.readlines()\n",
    "    cost = int(lkhout[1].strip().split(' ')[-1])\n",
    "\n",
    "    return [int(x)-1 for x in lkhout[6:-2:1]], cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TSP solution with spacing cost 113 has been found for the 851 non-expensive nodes\n"
     ]
    }
   ],
   "source": [
    "c,cost = run()\n",
    "\n",
    "print(f'A TSP solution with spacing cost {cost} has been found for the {len(filesmall)} non-expensive nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hypernode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = [ix for ix,x in enumerate(c) if x == len(c)-1][0]\n",
    "p = c[ix+1:] + c[:ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorten path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 770 words ladder with spacing cost 100 has been found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'haem idem idea iced ired prez prep peep weep weer deer dyer eyer ewer twee tree gree bree free flee fere fare fard lard lari lars lacs lack jack jock mock yock rock rocs mocs macs maes maps mads muds mugs migs mils oils olds elds elks ilks ills ells eels mels mems mess moss foss fuss buss cuss coss coys coos cots tots bots buts butt bute byte eyne sync syne syce sice fice fico piso miso mist wist wisp wiry wary warm farm barm bark sark dark dank yank oink kink kino kine fine bine bike bize bile bale dale dals daks oaks oafs kafs kifs kids kirs kiss hiss hose poke pole pele hebe heme heck keck kept wept weft left loft coft soft sift gift girt airt airn tirl tiro tyro typo tyre tare tarp harp hasp hash hush tush push pugh pugs purs purl burl furl farl harl hail heil deil debt deet leet leer jeer peer peel keel reel reek geek geed deed dees zees rees rues dues dubs duos dups sups suds sums sumo such ouch orcs orca orra okra aura jura jury fury fumy mumm mumu muni mini minx miff biff tiff teff toff tofu toit tort torr tory tony pony posy rosy nosy nogg hogg hogs hows bows pows pois phiz whiz whin shin sain said paid plod plop clop clap crap wrap wren when what phat phut pout gout goal goat doat moat mott mote dote doge dope nope mope more kore yowe yows yobs yods hods nods nodi nori nevi neve gyve gave gane sane safe sloe aloe alme ante ants ands anas anal anil anoa ansa atma alma alba albs alls ally olla olea plea pupa puna pina pica mica lira lire life lice lick tick tuck yuck puck punk gunk sunk sunn sung song sang bang bong bung bund buns nuns nans nabs nabe nada nana nona none zone zonk zoon noon neon neap heap heat beat feat felt gelt celt cell yell yill rill rial sial scab scan swan hwan moan mown sown sorn sort wort wost post psst pest nest neem neum scum scud spun stun stey stew slew skew skeg skip slip flip flit flat flak flam flog frog trop trod trad tray fray pray play plat prat prao pram gram grim trim drib drub daub chub chum cham chay chow chaw craw braw bras bros arfs arbs urbs eras ekes okes okeh oxen omen omer emir emic amid avid acid arid aria brin brig bait bast wast east vast oast oust rust just yurt hurt curt curb carb card bard barn darn durn curn cure cute cete cere cero pert part pare para vara java fava maya raya rays lays kays days deys beys buys bugs begs bigs bins ains yins rins sins sims sits sith site sire cire hire hive five file film fila gala gale gate gats guts cuts cats pats pads tads taps tape tace tame tome time mime dime dike dyke duke dude dado dago sago sadi sari shri soli sole mole mule mull mall malt molt mola moly mony many mano mane maze mazy hazy haze hade jade lade lame lace lane late lath bath beth both bota iota jota jots joys jogs jigs gigs gibs libs lips nips asps amps imps emus ecus plus plum slum slub snub sibb jibb jimp limp lamp gamp gulp gull gill gild sild wild weld weed teed toed toes voes vies gies gaes gars gabs jabs sabs says ways waps waws wawl wall will pill pili pily paly pals sals salp salt silt jilt riot root room poor boor bomb boob boot soot sook soak spaz spay slay slag smug snug snog snag snap seam beam dean dead dyad kyak kyar kbar koas boas baas bams baps zaps zags zigs rigs pigs pics pits kits kite wite lite nite nixe nide vide ride rile ripe rite ritz ditz dits dies diet duet duel dull doll boll bill bilk bulk hulk murk mark marc mask bask bash dash pash posh gosh qoph koph kaph kaka kata kats qats mats vats vets gets lets legs lugs jugs vugs vugg viga vina viny liny lint tint ting zing hind rind rand wand wyns wins wigs wogs wops sops oops lops lots loti roti rote rode code mode made mage magi ragi rami raki wake ware yare rare rase vase lase lyse lose love lobe lobo load road roar rear pear peag peps ceps cess jess jets fets feds revs devs dews dewy deny vend vent cent rent sent sene gene gone gong dong dons'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_running_cost(p,file):\n",
    "    check = []\n",
    "    mx = len(p)\n",
    "    for ix,x,y in zip(range(mx),p,p[1:]):\n",
    "        distance = spacing(file[x],file[y])\n",
    "        if distance:\n",
    "            check.append((ix,x,y,distance))\n",
    "    \n",
    "    result = []\n",
    "    for (ix,ax,bx,cx),(iy,ay,by,cy) in zip(check[:-2],check[1:]):\n",
    "        result.append((ix,iy-ix, bx,ay))\n",
    "            \n",
    "    return result\n",
    "\n",
    "result = find_running_cost(p,filesmall)\n",
    "\n",
    "a = 0\n",
    "b = len(c)\n",
    "\n",
    "while cost > 100+2:\n",
    "    if result[0][1] > result[-1][1]:\n",
    "        a = result[0][0]\n",
    "        del result[0]\n",
    "    else:\n",
    "        b = result[-1][0]\n",
    "        del result[-1]\n",
    "    cost -= 1\n",
    "    \n",
    "r = p[a:b]\n",
    "\n",
    "print(f'A {len(r)} words ladder with spacing cost 100 has been found')\n",
    "\n",
    "' '.join([filesmall[x] for x in r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([spacing(filesmall[x],filesmall[y]) for x,y in zip(r,r[1:])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
